# -*- coding: utf-8 -*-
"""OMDENA_Customer_Intent_Classification_Augmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kaa_DgJ-MqCFq05rruc8GCOJ4RkiGG-D
"""

! pip install transformers==4.28.0
! pip install accelerate
! pip install datasets
! pip install nltk
! pip install huggingface_hub
! pip install sentencepiece
! pip install evaluate
! pip install gdown
! pip install git-lfs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string

import warnings
warnings.filterwarnings('ignore')

import nltk
nltk.download('punkt')

"""# Load Datasets"""

from datasets import load_dataset
dataset = load_dataset("bitext/customer-support-intent-dataset")

print(dataset)

# convert datasetdict to pandas format
train = dataset["train"].to_pandas()
validation = dataset["validation"].to_pandas()
test = dataset["test"].to_pandas()

train

validation

test

"""## Save to csv format"""

train.to_csv('/content/drive/MyDrive/OMDENA DATASET/df_train.csv', index=False)

validation.to_csv('/content/drive/MyDrive/OMDENA DATASET/df_validation.csv', index=False)

test.to_csv('/content/drive/MyDrive/OMDENA DATASET/df_test.csv', index=False)

"""# Exploratory Data Analysis (EDA)"""

df = pd.read_csv('/content/drive/MyDrive/OMDENA DATASET/df_train_augmentasi.csv')

df

df.info()

df['length'] = df['utterance'].apply(lambda x: len(x.split()))

df

# check descriptive statistics for numeric column

df.describe(exclude='object').T

"""from result we know the minimum length of utterance is 1 and the maximum length is 15"""

df[df['length'] == 1]

df[df['length'] == 15]

# check descriptive statistics for categorical column

df.describe(exclude='number').T

"""from result we know the intent of get_invoice has maximum frequency and the category of ACCOUNT has the maximum frequency"""

# check for NaN value

df.isna().sum()

# check for duplicated value

df.duplicated(subset='utterance').sum()

"""## Category"""

# get the 5 highest amount of data from each category

custom_order = df.category.value_counts().nlargest(5).index

strong_color = '#FF5733'

palette = [strong_color if cat == custom_order[0] else sns.color_palette("magma")[0] for cat in custom_order]

plt.figure(figsize=(15, 10))
plt.title('the 5 highest amount of data from each category')
sns.countplot(data=df, y='category', order=custom_order, palette=palette)

ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_width() + 2, p.get_y() + p.get_height() / 2, int(p.get_width()), ha='left', va='center', fontsize=10, color='black')

plt.show()

"""the account category has the highest number of data frequencies. Let's check what intents have the most frequency in the ACCOUNT category"""

custom_order = df.intent[df['category']=='ACCOUNT'].value_counts().index

strong_color = '#FF5733'

palette = [strong_color if cat == custom_order[0] else sns.color_palette("magma")[0] for cat in custom_order]

plt.figure(figsize=(15, 10))
plt.title('the amount of data from each intent in ACCOUNT category')
sns.countplot(data=df, y=df.intent[df['category']=='ACCOUNT'], order=custom_order, palette=palette)

ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_width() + 2, p.get_y() + p.get_height() / 2, int(p.get_width()), ha='left', va='center', fontsize=10, color='black')


plt.show()

"""in the category of ACCOUNT, recover_password has the most frequency

## Intent
"""

custom_order = df.intent.value_counts().index

# make a df for only maximum and minimum intent count
custom_df = df[df['intent'].isin([custom_order[0], custom_order[-1]])].reset_index(drop=True)

plt.figure(figsize=(15, 10))
plt.title('Maximum and Minimum Intent Counts')
sns.countplot(data=custom_df, x='intent', hue='category', order=[custom_order[0], custom_order[-1]])

ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_x() + p.get_width() / 2, p.get_height() + 2, int(p.get_height()), ha='center', va='bottom', fontsize=10, color='black')

plt.xticks(rotation=45)
plt.show()

"""based on the plot, we know that the intent with the highest frequency is get_invoice and the lowest is contact_human_agent whivh belong to the category of INVOICE and CONTACT respectively"""

for i,j in enumerate(df.intent.unique()):

  print('{}. INTENT: {} CATEGORY: {} \n'.format(i+1, j, df['category'][df.intent == j].unique()))

"""## Wordcloud"""

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.corpus import stopwords

nltk.download('stopwords')

stpwrds = set(STOPWORDS)

for i in stopwords.words('english'):

  if i not in stpwrds:
    stpwrds.update([i])

wordCloud_data = df.copy()
wordCloud_data['utterance'] = wordCloud_data['utterance'].apply(lambda x: x.lower())

def word_cloud(data):

  text = ' '.join([text for text in data])

  # Create stopword list:
  stopwords = set(stpwrds)

  # Generate a word cloud image
  wordcloud = WordCloud(stopwords=stopwords, background_color="white", collocations = False).generate(text)

  # Display the generated image:
  # the matplotlib way:
  plt.figure(figsize=(15,10))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis("off")
  plt.show()

  text_dictionary = wordcloud.process_text(text)
  # sort the dictionary
  word_freq={k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}

  #use words_ to print relative word frequencies
  #rel_freq=wordcloud.words_

  #print results
  print(f'\n')
  print(f'most frequency of word:')
  print(list(word_freq.items())[:10])
  print(f'\n')
  print(f'least frequency of word:')
  print(list(word_freq.items())[-30:-1])

"""### ORDER"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='ORDER'].copy().reset_index(drop=True)

word_cloud(data)

"""### SHIPPING ADDRESS"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='SHIPPING_ADDRESS'].copy().reset_index(drop=True)

word_cloud(data)

"""### CANCELLATION FEE"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='CANCELLATION_FEE'].copy().reset_index(drop=True)

word_cloud(data)

"""### INVOICE"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='INVOICE'].copy().reset_index(drop=True)

word_cloud(data)

"""### PAYMENT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='PAYMENT'].copy().reset_index(drop=True)

word_cloud(data)

"""### REFUND"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='REFUND'].copy().reset_index(drop=True)

word_cloud(data)

"""### FEEDBACK"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='FEEDBACK'].copy().reset_index(drop=True)

word_cloud(data)

"""### CONTACT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='CONTACT'].copy().reset_index(drop=True)

word_cloud(data)

"""### ACCOUNT"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='ACCOUNT'].copy().reset_index(drop=True)

word_cloud(data)

"""### DELIVERY"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='DELIVERY'].copy().reset_index(drop=True)

word_cloud(data)

"""### NEWSLETTER"""

data = wordCloud_data['utterance'][wordCloud_data['category']=='NEWSLETTER'].copy().reset_index(drop=True)

word_cloud(data)

"""### statistical test between intent and category"""

from scipy.stats import chi2_contingency
from scipy.stats import chi2


chi2_check = {}


prob = 0.95
chi, p, dof, ex = chi2_contingency(pd.crosstab(df['category'], df['intent']))
chi2_check.setdefault('Feature',[]).append('category')
chi2_check.setdefault('chi',[]).append(chi)
chi2_check.setdefault('critical 0.05',[]).append(chi2.ppf(prob, dof))
chi2_check.setdefault('p-value',[]).append(round(p, 10))
chi2_check.setdefault('alpha',[]).append('0.05')

chi2_result = pd.DataFrame(data = chi2_check)
chi2_result.sort_values(by = ['p-value'], ascending = True, ignore_index = True, inplace = True)
chi2_result

"""*   If Statistic >= Critical Value: significant result, reject null hypothesis (H0), dependent.
*   If Statistic < Critical Value: not significant result, fail to reject null hypothesis (H0), independent.

*   If p-value <= alpha: significant result, reject null hypothesis (H0), dependent.
*   If p-value > alpha: not significant result, fail to reject null hypothesis (H0), independent.

Based on the Chi2 test, it can be seen that the category variable has a significant influence on the target intent variable. We can use the category variable to use in modeling if needed.

# Preprocessing
"""

!pip install contractions

import pandas as pd
import numpy as np
import re
import math
import string
import unicodedata

from nltk.stem import PorterStemmer
import contractions

from sklearn.base import BaseEstimator, TransformerMixin

contraction = {key.lower(): value.lower() for key, value in contractions.contractions_dict.items()}

class Prepro(BaseEstimator, TransformerMixin):

    def normalize(self, text):
        text = text.lower()
        text = text.split()

        for idx, word in enumerate(text):

          if word in list(contraction.keys()):
            text[idx] = contraction[word]

        return ' '.join(text)

    def cleansing(self, text):

        text = re.sub(r"\s+", " ", text, flags=re.UNICODE)
        text = re.sub(r'[0-9]', '', text) # number
        text = (unicodedata.normalize("NFD", text).encode("ascii", "ignore").decode("ascii"))
        text = re.sub(r'[\!\"\”\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\[\\\]\^\_\`\{\|\}\~\–]', '', text) #punctuation
        text = re.sub(r' +', ' ', text)
        text = text.strip()

        return text.lower()

    def stopwords(self, text):

        text = text.split()

        text = [word for word in text if word not in stpwrds]

        return ' '.join(text)

    def stemming(self, text):

        text = text.split()
        stemmer = PorterStemmer()

        return ' '.join([stemmer.stem(word) for word in text])

    def fit(self, X, y=None):
        return self

    def transform(self, X):

        X_copy = X.copy()  # Make a copy to avoid modifying the original DataFrame
        X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.normalize(x))
        X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.cleansing(x))
        X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stopwords(x))
        X_copy['utterance'] = X_copy['utterance'].apply(lambda x: self.stemming(x))

        return X_copy

train_df = df.copy()
valid_df = pd.read_csv('/content/drive/MyDrive/OMDENA DATASET/df_validation.csv')

preprocessor = Prepro()

train_df = preprocessor.fit_transform(train_df)

valid_df = preprocessor.transform(valid_df)

train_df

valid_df

from sklearn.model_selection import train_test_split

train_df, valid_df_new = train_test_split(train_df, test_size=0.2, stratify=train_df.intent, random_state=42)

train_df = train_df.reset_index(drop=True)
valid_df_new = valid_df_new.reset_index(drop=True)

valid_df = pd.concat([valid_df, valid_df_new], ignore_index=True, sort=False)

valid_df.isna().sum()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_df['intent'] = le.fit_transform(train_df['intent'])
valid_df['intent'] = le.transform(valid_df['intent'])

train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True) #shuffle df

valid_df = valid_df.sample(frac=1, random_state=42).reset_index(drop=True)

keys = le.classes_
values = le.transform(le.classes_)
dictionary = dict(zip(keys, values))
print(dictionary)

re = {v:k for k,v in dictionary.items()}
print(re)

from huggingface_hub import notebook_login

notebook_login()

train = train_df.rename(columns={'utterance': 'text', 'intent': 'label'})

valid = valid_df.rename(columns={'utterance': 'text', 'intent': 'label'})

train = train.drop(columns=['category', 'length'])
valid = valid.drop(columns=['category', 'tags', 'length'])

from datasets import Dataset

train_dataset = Dataset.from_dict(train)
valid_dataset = Dataset.from_dict(valid)

import datasets
dd = datasets.DatasetDict({"train":train_dataset,"validation":valid_dataset})

"""# Modeling"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length')

tokenized_datasets = dd.map(tokenize_function, batched=True)

print(tokenized_datasets['train'][0])

tokenized_datasets

tokenized_datasets = tokenized_datasets.remove_columns(
    'text'
)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

data_collator(tokenized_datasets["train"][0])

import evaluate
import numpy as np

def compute_metrics(eval_pred):
    metric1 = evaluate.load("accuracy")
    metric2 = evaluate.load("precision")
    metric3 = evaluate.load("recall")

    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = metric1.compute(predictions=predictions, references=labels)['accuracy']
    precision = metric2.compute(predictions=predictions, references=labels, average='macro')["precision"]
    recall = metric3.compute(predictions=predictions, references=labels, average='macro')["recall"]

    return {"accuracy": accuracy, "precision": precision, "recall": recall}

id2label = {0: 'cancel_order', 1: 'change_order', 2: 'change_shipping_address', 3: 'check_cancellation_fee', 4: 'check_invoice', 5: 'check_payment_methods', 6: 'check_refund_policy', 7: 'complaint', 8: 'contact_customer_service', 9: 'contact_human_agent', 10: 'create_account', 11: 'delete_account', 12: 'delivery_options', 13: 'delivery_period', 14: 'edit_account', 15: 'get_invoice', 16: 'get_refund', 17: 'newsletter_subscription', 18: 'payment_issue', 19: 'place_order', 20: 'recover_password', 21: 'registration_problems', 22: 'review', 23: 'set_up_shipping_address', 24: 'switch_account', 25: 'track_order', 26: 'track_refund'}
label2id = {'cancel_order': 0, 'change_order': 1, 'change_shipping_address': 2, 'check_cancellation_fee': 3, 'check_invoice': 4, 'check_payment_methods': 5, 'check_refund_policy': 6, 'complaint': 7, 'contact_customer_service': 8, 'contact_human_agent': 9, 'create_account': 10, 'delete_account': 11, 'delivery_options': 12, 'delivery_period': 13, 'edit_account': 14, 'get_invoice': 15, 'get_refund': 16, 'newsletter_subscription': 17, 'payment_issue': 18, 'place_order': 19, 'recover_password': 20, 'registration_problems': 21, 'review': 22, 'set_up_shipping_address': 23, 'switch_account': 24, 'track_order': 25, 'track_refund': 26}

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=27, id2label=id2label, label2id=label2id
)

from transformers import TrainerCallback, TrainerControl

class LossEqualityCallback(TrainerCallback):

    def __init__(self, max_loss_diff=0.005):
        self.max_loss_diff = max_loss_diff

    # try to stop the training process in a non-overfit condition
    def on_log(self, args, state, control, val={}, **kwargs):

        if 'logs' in kwargs:

          if 'loss' in kwargs['logs']:
            val.setdefault('loss', []).append(kwargs['logs']['loss'])
          elif 'eval_loss' in kwargs['logs']:
            val.setdefault('eval_loss', []).append(kwargs['logs']['eval_loss'])

        if 'loss' in val and 'eval_loss' in val:

          if len(val['loss']) == len(val['eval_loss']):
            if val['loss'][-1] < val['eval_loss'][-1] and abs(val['loss'][-1] - val['eval_loss'][-1]) > self.max_loss_diff:

              control.should_training_stop = True
              print('TRAINING STOP')
              print(f'val: {val}')
              print(state.log_history)

custom_callback = LossEqualityCallback()

batch_size = 32
num_train_epochs = 10
logging_steps = len(tokenized_datasets["train"]) // batch_size

training_args = TrainingArguments(
    output_dir='distilbert-base-uncased-OMDENA-cllbck-augmentation',
    learning_rate=2e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    weight_decay=0.0001,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
    logging_steps=logging_steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[custom_callback]
)

trainer.train()

trainer.evaluate()

trainer.push_to_hub()

trainer.save_model("/content/drive/MyDrive/intent-classification-OMDENA-cllbck-augmentation")

trainer.state.log_history[:-2]

loss = []
val_loss = []
epoch = []

for i in trainer.state.log_history[:-2]:

  if 'loss' in i:
    loss.append(i['loss'])
  elif 'eval_loss' in i:
    val_loss.append(i['eval_loss'])

for j in trainer.state.log_history[:-2]:

  if 'epoch' in j:
    epoch.append(round(j['epoch']))

epoch = list(set(epoch))

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

data = pd.DataFrame({'Epochs': epoch, 'Loss': loss, 'Validation Loss': val_loss})

# Create a line plot using Seaborn
sns.set(style="whitegrid")  # Set the style of the plot
plt.figure(figsize=(10, 6))  # Set the size of the plot
plot = sns.lineplot(data=data, x='Epochs', y='Loss', label='Loss')
plot = sns.lineplot(data=data, x='Epochs', y='Validation Loss', label='Validation Loss')

# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss and Validation Loss Over Epochs')

# Show legend
plt.legend()

# Show the plot
plt.show()

"""# Inference"""

!pip install contractions

import pandas as pd
import numpy as np
import re
import string
import unicodedata

import nltk
from nltk.stem import PorterStemmer
import contractions

from sklearn.base import BaseEstimator, TransformerMixin

from wordcloud import STOPWORDS
from nltk.corpus import stopwords

nltk.download('punkt')
nltk.download('stopwords')

import warnings
warnings.filterwarnings('ignore')

contraction = {key.lower(): value.lower() for key, value in contractions.contractions_dict.items()}

stpwrds = set(STOPWORDS)

for i in stopwords.words('english'):

  if i not in stpwrds:
    stpwrds.update([i])

class Prepro(BaseEstimator, TransformerMixin):

    def normalize(self, text):
        text = text.lower()
        text = text.split()

        for idx, word in enumerate(text):

          if word in list(contraction.keys()):
            text[idx] = contraction[word]

        return ' '.join(text)

    def cleansing(self, text):

        text = re.sub(r"\s+", " ", text, flags=re.UNICODE)
        text = re.sub(r'[0-9]', '', text) # number
        text = (unicodedata.normalize("NFD", text).encode("ascii", "ignore").decode("ascii"))
        text = re.sub(r'[\!\"\”\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\[\\\]\^\_\`\{\|\}\~\–]', '', text) #punctuation
        text = re.sub(r' +', ' ', text)
        text = text.strip()

        return text.lower()

    def stopwords(self, text):

        text = text.split()

        text = [word for word in text if word not in stpwrds]

        return ' '.join(text)

    def stemming(self, text):

        text = text.split()
        stemmer = PorterStemmer()

        return ' '.join([stemmer.stem(word) for word in text])

    def fit(self, X, y=None):
        return self

    def transform(self, X):

        X = self.normalize(X)
        X = self.cleansing(X)
        X = self.stopwords(X)
        X = self.stemming(X)

        return X

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/MyDrive/intent-classification-OMDENA-cllbck-augmentation")

tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/intent-classification-OMDENA-cllbck-augmentation")

id2label = {0: 'cancel_order', 1: 'change_order', 2: 'change_shipping_address', 3: 'check_cancellation_fee', 4: 'check_invoice', 5: 'check_payment_methods', 6: 'check_refund_policy', 7: 'complaint', 8: 'contact_customer_service', 9: 'contact_human_agent', 10: 'create_account', 11: 'delete_account', 12: 'delivery_options', 13: 'delivery_period', 14: 'edit_account', 15: 'get_invoice', 16: 'get_refund', 17: 'newsletter_subscription', 18: 'payment_issue', 19: 'place_order', 20: 'recover_password', 21: 'registration_problems', 22: 'review', 23: 'set_up_shipping_address', 24: 'switch_account', 25: 'track_order', 26: 'track_refund'}

def inference(text, id2label=id2label):
  preprocessing = Prepro()

  for idx, sentence in enumerate(text):

    sentence = preprocessing.fit_transform(sentence)

    encoding = tokenizer(sentence, return_tensors="pt")
    outputs = model(**encoding)
    predictions = int(outputs.logits.argmax(-1))

    text[idx] = id2label[predictions]

  return text

text = ["i want to change my shipping address","help for create account", "can you change the number i have?", "sorry your service is so bad", "how to get back my money"]

inference(text)

"""# Hyperparameter Tuning"""

! pip install ray[tune]

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig

db_config_base = AutoConfig.from_pretrained("distilbert-base-uncased")

db_config_base

from ray import tune

def model_init(params):
        db_config = db_config_base
        if params is not None:
            db_config.update({'dropout': params['dropout'],'num_labels':43, 'id2label':id2label, 'label2id':label2id})
        return AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", config=db_config)

def hp_space_ray(trial):
    return {
        "learning_rate": tune.choice([2e-5, 2e-4, 2e-3]),
        "per_device_train_batch_size": tune.choice([8, 16, 24, 32]),
        "per_device_eval_batch_size":tune.choice([8, 16, 24, 32]),
        "weight_decay": tune.choice([1e-4, 1e-3, 1e-2]),
        "dropout" : tune.choice([0.1, 0.2, 0.3, 0.4])
    }

training_args = TrainingArguments(
    output_dir='distilbert-base-uncased-OMDENA-cllbck-augmentation',
    evaluation_strategy='epoch',
    eval_steps=500,
    gradient_accumulation_steps=1000,
    eval_accumulation_steps=1
)

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

best_trial = trainer.hyperparameter_search(
    hp_space=hp_space_ray,
    direction="maximize",
    backend="ray",
    n_trials=7)

print(best_trial.hyperparameters.items())

trainer.args

import json

# create an example dictionary
best_parameter = dict(best_trial.hyperparameters.items())

# write the dictionary to a JSON file
with open('/content/drive/MyDrive/OMDENA DATASET/raytune_omdena.json', 'w') as f:
    json.dump(best_parameter, f)